import torch
import cv2
import numpy as np
import os
from torchvision import transforms
from model import ConvNet
from config import Config
from utils import pad_resize_digit


def predict_image(image_path):
    # --- 1. åŠ è½½æ¨¡å‹ ---
    device = Config.DEVICE
    model = ConvNet().to(device)
    try:
        model.load_state_dict(torch.load(Config.MODEL_PATH, map_location=device))
    except FileNotFoundError:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {Config.MODEL_PATH}")
        return
    model.eval()

    # --- 2. è¯»å–å›¾ç‰‡ ---
    img = cv2.imread(image_path)
    if img is None:
        print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡: {image_path}")
        return

    # ä¸ºäº†ç»Ÿä¸€å¤„ç†æ ‡å‡†ï¼Œå…ˆæŠŠå›¾ç‰‡é«˜åº¦ Resize åˆ° 1000 åƒç´ 
    h, w = img.shape[:2]
    scale_ratio = 1000 / h
    new_w = int(w * scale_ratio)
    img = cv2.resize(img, (new_w, 1000))
    img_display = img.copy() 

    # --- 3. å›¾åƒé¢„å¤„ç† ---
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # é«˜æ–¯æ¨¡ç³Šï¼šå»é™¤çº¸å¼ çš„å™ªç‚¹é¢—ç²’
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)

    # è‡ªé€‚åº”é˜ˆå€¼ï¼šåº”å¯¹å…‰ç…§ä¸å‡åŒ€
    thresh = cv2.adaptiveThreshold(blurred, 255,
                                   cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                   cv2.THRESH_BINARY_INV, 19, 10)

    # å½¢æ€å­¦æ“ä½œï¼šé—­è¿ç®—ï¼ˆè¿æ¥æ–­å¼€çš„ç¬”ç”»ï¼‰
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)

    # --- 4. è½®å»“æå–ä¸ç­›é€‰ ---
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    digit_rects = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)

        # è®¡ç®—å®½é«˜æ¯”
        aspect_ratio = float(w) / h
        area = cv2.contourArea(cnt)

        # æ›´ä¸¥æ ¼çš„ç­›é€‰æ¡ä»¶
        if area > 400 and h > 30 and aspect_ratio < 1.5:
            digit_rects.append((x, y, w, h))

    # ä»å·¦åˆ°å³æ’åº
    digit_rects.sort(key=lambda x: x[0])

    # --- 5. é¢„æµ‹ ---
    result_str = ""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(Config.NORM_MEAN, Config.NORM_STD)
    ])

    print(f"ğŸ” è¿‡æ»¤å™ªç‚¹åï¼Œæ£€æµ‹åˆ° {len(digit_rects)} ä¸ªæœ‰æ•ˆæ•°å­—...")

    for i, (x, y, w, h) in enumerate(digit_rects):
        roi = thresh[y:y + h, x:x + w]

        # é’ˆå¯¹ç»†é•¿å­—ä½“è¿›è¡ŒåŠ ç²—ï¼Œé˜²æ­¢ resize åç‰¹å¾æ¶ˆå¤±
        kernel_dilate = np.ones((2, 2), np.uint8)
        roi = cv2.dilate(roi, kernel_dilate, iterations=1)

        roi_processed = pad_resize_digit(roi)

        img_tensor = transform(roi_processed).unsqueeze(0).to(device)

        with torch.no_grad():
            output = model(img_tensor)
            prediction = output.argmax(dim=1).item()
            result_str += str(prediction)

        # åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶ç»“æœ
        cv2.rectangle(img_display, (x, y), (x + w, y + h), (0, 255, 0), 3)  # åŠ ç²—è¾¹æ¡†
        cv2.putText(img_display, str(prediction), (x, y - 15),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 0, 255), 4)  # åŠ å¤§å­—ä½“

    # --- 6. æ˜¾ç¤ºå’Œä¿å­˜ç»“æœ ---
    print("\n" + "=" * 40)
    print(f"ğŸ“¸ åŸå§‹æ–‡ä»¶: {image_path}")
    print(f"ğŸ”¢ è¯†åˆ«ç»“æœ: {result_str}")
    print("=" * 40)

    # --- 7. è‡ªåŠ¨ä¿å­˜ç»“æœå›¾ç‰‡åˆ°åŒç›®å½• ---
    # ç”Ÿæˆä¿å­˜è·¯å¾„ï¼ˆåœ¨åŸæ–‡ä»¶åŒç›®å½•ï¼Œæ–‡ä»¶åæ·»åŠ _resultåç¼€ï¼‰
    dir_name = os.path.dirname(image_path)
    base_name = os.path.basename(image_path)
    file_name, ext = os.path.splitext(base_name)
    save_path = os.path.join(dir_name, f"{file_name}_result{ext}")
    
    # åœ¨å›¾ç‰‡é¡¶éƒ¨æ·»åŠ è¯†åˆ«ç»“æœæ–‡æœ¬
    result_text = f"Result: {result_str}"
    text_size = cv2.getTextSize(result_text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 3)[0]
    text_x = (img_display.shape[1] - text_size[0]) // 2
    text_y = 50
    
    # æ·»åŠ åŠé€æ˜èƒŒæ™¯
    overlay = img_display.copy()
    cv2.rectangle(overlay, (text_x - 10, text_y - 40), 
                 (text_x + text_size[0] + 10, text_y + 10), (200, 200, 200), -1)
    alpha = 0.7
    img_display = cv2.addWeighted(overlay, alpha, img_display, 1 - alpha, 0)
    
    # æ·»åŠ æ–‡å­—
    cv2.putText(img_display, result_text, (text_x, text_y),
                cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)
    
    # ä¿å­˜ç»“æœå›¾ç‰‡
    cv2.imwrite(save_path, img_display)
    print(f"âœ… ç»“æœå›¾ç‰‡å·²ä¿å­˜è‡³: {save_path}")

    # --- 8. æ˜¾ç¤ºæœ€ç»ˆç»“æœ ---
    display_h = 600
    display_ratio = display_h / img_display.shape[0]
    display_w = int(img_display.shape[1] * display_ratio)
    final_show = cv2.resize(img_display, (display_w, display_h))

    # æ·»åŠ çª—å£æ ‡é¢˜
    cv2.imshow(f"è¯†åˆ«ç»“æœ: {result_str} | æŒ‰ä»»æ„é”®å…³é—­", final_show)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


if __name__ == '__main__':
    predict_image('test.jpg') 